{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/yuezhihan/ts2vec.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Qw-VlY_N9bm",
    "outputId": "f2900e3d-33ad-4e59-a0f3-519063abdf71"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'ts2vec' already exists and is not an empty directory.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLjxa4UaLSGh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from ts2vec.models import TSEncoder\n",
    "from ts2vec.models.losses import hierarchical_contrastive_loss\n",
    "from ts2vec.utils import take_per_row, split_with_nan, centerize_vary_length_series, torch_pad_nan\n",
    "import math\n",
    "\n",
    "class TS2Vec:\n",
    "    '''The TS2Vec model'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims=320,\n",
    "        hidden_dims=64,\n",
    "        depth=10,\n",
    "        device='cuda',\n",
    "        lr=0.001,\n",
    "        batch_size=16,\n",
    "        max_train_length=None,\n",
    "        temporal_unit=0,\n",
    "        after_iter_callback=None,\n",
    "        after_epoch_callback=None\n",
    "    ):\n",
    "        ''' Initialize a TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            input_dims (int): The input dimension. For a univariate time series, this should be set to 1.\n",
    "            output_dims (int): The representation dimension.\n",
    "            hidden_dims (int): The hidden dimension of the encoder.\n",
    "            depth (int): The number of hidden residual blocks in the encoder.\n",
    "            device (int): The gpu used for training and inference.\n",
    "            lr (int): The learning rate.\n",
    "            batch_size (int): The batch size.\n",
    "            max_train_length (Union[int, NoneType]): The maximum allowed sequence length for training. For sequence with a length greater than <max_train_length>, it would be cropped into some sequences, each of which has a length less than <max_train_length>.\n",
    "            temporal_unit (int): The minimum unit to perform temporal contrast. When training on a very long sequence, this param helps to reduce the cost of time and memory.\n",
    "            after_iter_callback (Union[Callable, NoneType]): A callback function that would be called after each iteration.\n",
    "            after_epoch_callback (Union[Callable, NoneType]): A callback function that would be called after each epoch.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.max_train_length = max_train_length\n",
    "        self.temporal_unit = temporal_unit\n",
    "        \n",
    "        self._net = TSEncoder(input_dims=input_dims, output_dims=output_dims, hidden_dims=hidden_dims, depth=depth).to(self.device)\n",
    "        self.net = torch.optim.swa_utils.AveragedModel(self._net)\n",
    "        self.net.update_parameters(self._net)\n",
    "        \n",
    "        self.after_iter_callback = after_iter_callback\n",
    "        self.after_epoch_callback = after_epoch_callback\n",
    "        \n",
    "        self.n_epochs = 0\n",
    "        self.n_iters = 0\n",
    "    \n",
    "    def fit(self, train_data, n_epochs=None, n_iters=None, verbose=False):\n",
    "        ''' Training the TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            train_data (numpy.ndarray): The training data. It should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            n_epochs (Union[int, NoneType]): The number of epochs. When this reaches, the training stops.\n",
    "            n_iters (Union[int, NoneType]): The number of iterations. When this reaches, the training stops. If both n_epochs and n_iters are not specified, a default setting would be used that sets n_iters to 200 for a dataset with size <= 100000, 600 otherwise.\n",
    "            verbose (bool): Whether to print the training loss after each epoch.\n",
    "            \n",
    "        Returns:\n",
    "            loss_log: a list containing the training losses on each epoch.\n",
    "        '''\n",
    "        assert train_data.ndim == 3\n",
    "        \n",
    "        if n_iters is None and n_epochs is None:\n",
    "            n_iters = 200 if train_data.size <= 100000 else 600  # default param for n_iters\n",
    "        \n",
    "        if self.max_train_length is not None:\n",
    "            sections = train_data.shape[1] // self.max_train_length\n",
    "            if sections >= 2:\n",
    "                train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
    "\n",
    "        temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
    "        if temporal_missing[0] or temporal_missing[-1]:\n",
    "            train_data = centerize_vary_length_series(train_data)\n",
    "                \n",
    "        train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
    "        \n",
    "        train_dataset = TensorDataset(torch.from_numpy(train_data).to(torch.float))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=min(self.batch_size, len(train_dataset)), shuffle=True, drop_last=True)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self._net.parameters(), lr=self.lr)\n",
    "        \n",
    "        loss_log = []\n",
    "        \n",
    "        while True:\n",
    "            if n_epochs is not None and self.n_epochs >= n_epochs:\n",
    "                break\n",
    "            \n",
    "            cum_loss = 0\n",
    "            n_epoch_iters = 0\n",
    "            \n",
    "            interrupted = False\n",
    "            for batch in train_loader:\n",
    "                if n_iters is not None and self.n_iters >= n_iters:\n",
    "                    interrupted = True\n",
    "                    break\n",
    "                \n",
    "                x = batch[0]\n",
    "                if self.max_train_length is not None and x.size(1) > self.max_train_length:\n",
    "                    window_offset = np.random.randint(x.size(1) - self.max_train_length + 1)\n",
    "                    x = x[:, window_offset : window_offset + self.max_train_length]\n",
    "                x = x.to(self.device)\n",
    "                \n",
    "                ts_l = x.size(1)\n",
    "                crop_l = np.random.randint(low=2 ** (self.temporal_unit + 1), high=ts_l+1)\n",
    "                crop_left = np.random.randint(ts_l - crop_l + 1)\n",
    "                crop_right = crop_left + crop_l\n",
    "                crop_eleft = np.random.randint(crop_left + 1)\n",
    "                crop_eright = np.random.randint(low=crop_right, high=ts_l + 1)\n",
    "                crop_offset = np.random.randint(low=-crop_eleft, high=ts_l - crop_eright + 1, size=x.size(0))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                out1 = self._net(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft))\n",
    "                out1 = out1[:, -crop_l:]\n",
    "                \n",
    "                out2 = self._net(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left))\n",
    "                out2 = out2[:, :crop_l]\n",
    "                \n",
    "                loss = hierarchical_contrastive_loss(\n",
    "                    out1,\n",
    "                    out2,\n",
    "                    temporal_unit=self.temporal_unit\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.net.update_parameters(self._net)\n",
    "                    \n",
    "                cum_loss += loss.item()\n",
    "                n_epoch_iters += 1\n",
    "                \n",
    "                self.n_iters += 1\n",
    "                \n",
    "                if self.after_iter_callback is not None:\n",
    "                    self.after_iter_callback(self, loss.item())\n",
    "            \n",
    "            if interrupted:\n",
    "                break\n",
    "            \n",
    "            cum_loss /= n_epoch_iters\n",
    "            loss_log.append(cum_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch #{self.n_epochs}: loss={cum_loss}\")\n",
    "            self.n_epochs += 1\n",
    "            \n",
    "            if self.after_epoch_callback is not None:\n",
    "                self.after_epoch_callback(self, cum_loss)\n",
    "            \n",
    "        return loss_log\n",
    "    \n",
    "    def _eval_with_pooling(self, x, mask=None, slicing=None, encoding_window=None):\n",
    "        out = self.net(x.to(self.device, non_blocking=True), mask)\n",
    "        if encoding_window == 'full_series':\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = out.size(1),\n",
    "            ).transpose(1, 2)\n",
    "            \n",
    "        elif isinstance(encoding_window, int):\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = encoding_window,\n",
    "                stride = 1,\n",
    "                padding = encoding_window // 2\n",
    "            ).transpose(1, 2)\n",
    "            if encoding_window % 2 == 0:\n",
    "                out = out[:, :-1]\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        elif encoding_window == 'multiscale':\n",
    "            p = 0\n",
    "            reprs = []\n",
    "            while (1 << p) + 1 < out.size(1):\n",
    "                t_out = F.max_pool1d(\n",
    "                    out.transpose(1, 2),\n",
    "                    kernel_size = (1 << (p + 1)) + 1,\n",
    "                    stride = 1,\n",
    "                    padding = 1 << p\n",
    "                ).transpose(1, 2)\n",
    "                if slicing is not None:\n",
    "                    t_out = t_out[:, slicing]\n",
    "                reprs.append(t_out)\n",
    "                p += 1\n",
    "            out = torch.cat(reprs, dim=-1)\n",
    "            \n",
    "        else:\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        return out.cpu()\n",
    "    \n",
    "    def encode(self, data, mask=None, encoding_window=None, casual=False, sliding_length=None, sliding_padding=0, batch_size=None):\n",
    "        ''' Compute representations using the model.\n",
    "        \n",
    "        Args:\n",
    "            data (numpy.ndarray): This should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            mask (str): The mask used by encoder can be specified with this parameter. This can be set to 'binomial', 'continuous', 'all_true', 'all_false' or 'mask_last'.\n",
    "            encoding_window (Union[str, int]): When this param is specified, the computed representation would the max pooling over this window. This can be set to 'full_series', 'multiscale' or an integer specifying the pooling kernel size.\n",
    "            casual (bool): When this param is set to True, the future informations would not be encoded into representation of each timestamp.\n",
    "            sliding_length (Union[int, NoneType]): The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "            sliding_padding (int): This param specifies the contextual data length used for inference every sliding windows.\n",
    "            batch_size (Union[int, NoneType]): The batch size used for inference. If not specified, this would be the same batch size as training.\n",
    "            \n",
    "        Returns:\n",
    "            repr: The representations for data.\n",
    "        '''\n",
    "        assert self.net is not None, 'please train or load a net first'\n",
    "        assert data.ndim == 3\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        n_samples, ts_l, _ = data.shape\n",
    "\n",
    "        org_training = self.net.training\n",
    "        self.net.eval()\n",
    "        \n",
    "        dataset = TensorDataset(torch.from_numpy(data).to(torch.float))\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for batch in loader:\n",
    "                x = batch[0]\n",
    "                if sliding_length is not None:\n",
    "                    reprs = []\n",
    "                    if n_samples < batch_size:\n",
    "                        calc_buffer = []\n",
    "                        calc_buffer_l = 0\n",
    "                    for i in range(0, ts_l, sliding_length):\n",
    "                        l = i - sliding_padding\n",
    "                        r = i + sliding_length + (sliding_padding if not casual else 0)\n",
    "                        x_sliding = torch_pad_nan(\n",
    "                            x[:, max(l, 0) : min(r, ts_l)],\n",
    "                            left=-l if l<0 else 0,\n",
    "                            right=r-ts_l if r>ts_l else 0,\n",
    "                            dim=1\n",
    "                        )\n",
    "                        if n_samples < batch_size:\n",
    "                            if calc_buffer_l + n_samples > batch_size:\n",
    "                                out = self._eval_with_pooling(\n",
    "                                    torch.cat(calc_buffer, dim=0),\n",
    "                                    mask,\n",
    "                                    slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                    encoding_window=encoding_window\n",
    "                                )\n",
    "                                reprs += torch.split(out, n_samples)\n",
    "                                calc_buffer = []\n",
    "                                calc_buffer_l = 0\n",
    "                            calc_buffer.append(x_sliding)\n",
    "                            calc_buffer_l += n_samples\n",
    "                        else:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                x_sliding,\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs.append(out)\n",
    "\n",
    "                    if n_samples < batch_size:\n",
    "                        if calc_buffer_l > 0:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                torch.cat(calc_buffer, dim=0),\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs += torch.split(out, n_samples)\n",
    "                            calc_buffer = []\n",
    "                            calc_buffer_l = 0\n",
    "                    \n",
    "                    out = torch.cat(reprs, dim=1)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = F.max_pool1d(\n",
    "                            out.transpose(1, 2).contiguous(),\n",
    "                            kernel_size = out.size(1),\n",
    "                        ).squeeze(1)\n",
    "                else:\n",
    "                    out = self._eval_with_pooling(x, mask, encoding_window=encoding_window)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = out.squeeze(1)\n",
    "                        \n",
    "                output.append(out)\n",
    "                \n",
    "            output = torch.cat(output, dim=0)\n",
    "            \n",
    "        self.net.train(org_training)\n",
    "        return output.numpy()\n",
    "    \n",
    "    def save(self, fn):\n",
    "        ''' Save the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        torch.save(self.net.state_dict(), fn)\n",
    "    \n",
    "    def load(self, fn):\n",
    "        ''' Load the model from a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        state_dict = torch.load(fn, map_location=self.device)\n",
    "        self.net.load_state_dict(state_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "def pkl_save(name, var):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(var, f)\n",
    "\n",
    "def pkl_load(name):\n",
    "    with open(name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def torch_pad_nan(arr, left=0, right=0, dim=0):\n",
    "    if left > 0:\n",
    "        padshape = list(arr.shape)\n",
    "        padshape[dim] = left\n",
    "        arr = torch.cat((torch.full(padshape, np.nan), arr), dim=dim)\n",
    "    if right > 0:\n",
    "        padshape = list(arr.shape)\n",
    "        padshape[dim] = right\n",
    "        arr = torch.cat((arr, torch.full(padshape, np.nan)), dim=dim)\n",
    "    return arr\n",
    "    \n",
    "def pad_nan_to_target(array, target_length, axis=0, both_side=False):\n",
    "    assert array.dtype in [np.float16, np.float32, np.float64]\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "    npad = [(0, 0)] * array.ndim\n",
    "    if both_side:\n",
    "        npad[axis] = (pad_size // 2, pad_size - pad_size//2)\n",
    "    else:\n",
    "        npad[axis] = (0, pad_size)\n",
    "    return np.pad(array, pad_width=npad, mode='constant', constant_values=np.nan)\n",
    "\n",
    "def split_with_nan(x, sections, axis=0):\n",
    "    assert x.dtype in [np.float16, np.float32, np.float64]\n",
    "    arrs = np.array_split(x, sections, axis=axis)\n",
    "    target_length = arrs[0].shape[axis]\n",
    "    for i in range(len(arrs)):\n",
    "        arrs[i] = pad_nan_to_target(arrs[i], target_length, axis=axis)\n",
    "    return arrs\n",
    "\n",
    "def take_per_row(A, indx, num_elem):\n",
    "    all_indx = indx[:,None] + np.arange(num_elem)\n",
    "    return A[torch.arange(all_indx.shape[0])[:,None], all_indx]\n",
    "\n",
    "def centerize_vary_length_series(x):\n",
    "    prefix_zeros = np.argmax(~np.isnan(x).all(axis=-1), axis=1)\n",
    "    suffix_zeros = np.argmax(~np.isnan(x[:, ::-1]).all(axis=-1), axis=1)\n",
    "    offset = (prefix_zeros + suffix_zeros) // 2 - prefix_zeros\n",
    "    rows, column_indices = np.ogrid[:x.shape[0], :x.shape[1]]\n",
    "    offset[offset < 0] += x.shape[1]\n",
    "    column_indices = column_indices - offset[:, np.newaxis]\n",
    "    return x[rows, column_indices]\n",
    "\n",
    "def data_dropout(arr, p):\n",
    "    B, T = arr.shape[0], arr.shape[1]\n",
    "    mask = np.full(B*T, False, dtype=np.bool)\n",
    "    ele_sel = np.random.choice(\n",
    "        B*T,\n",
    "        size=int(B*T*p),\n",
    "        replace=False\n",
    "    )\n",
    "    mask[ele_sel] = True\n",
    "    res = arr.copy()\n",
    "    res[mask.reshape(B, T)] = np.nan\n",
    "    return res\n",
    "\n",
    "def name_with_datetime(prefix='default'):\n",
    "    now = datetime.now()\n",
    "    return prefix + '_' + now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def init_dl_program(\n",
    "    device_name,\n",
    "    seed=None,\n",
    "    use_cudnn=True,\n",
    "    deterministic=False,\n",
    "    benchmark=False,\n",
    "    use_tf32=False,\n",
    "    max_threads=None\n",
    "):\n",
    "    import torch\n",
    "    if max_threads is not None:\n",
    "        torch.set_num_threads(max_threads)  # intraop\n",
    "        if torch.get_num_interop_threads() != max_threads:\n",
    "            torch.set_num_interop_threads(max_threads)  # interop\n",
    "        try:\n",
    "            import mkl\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            mkl.set_num_threads(max_threads)\n",
    "        \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        seed += 1\n",
    "        np.random.seed(seed)\n",
    "        seed += 1\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "    if isinstance(device_name, (str, int)):\n",
    "        device_name = [device_name]\n",
    "    \n",
    "    devices = []\n",
    "    for t in reversed(device_name):\n",
    "        t_device = torch.device(t)\n",
    "        devices.append(t_device)\n",
    "        if t_device.type == 'cuda':\n",
    "            assert torch.cuda.is_available()\n",
    "            torch.cuda.set_device(t_device)\n",
    "            if seed is not None:\n",
    "                seed += 1\n",
    "                torch.cuda.manual_seed(seed)\n",
    "    devices.reverse()\n",
    "    torch.backends.cudnn.enabled = use_cudnn\n",
    "    torch.backends.cudnn.deterministic = deterministic\n",
    "    torch.backends.cudnn.benchmark = benchmark\n",
    "    \n",
    "    if hasattr(torch.backends.cudnn, 'allow_tf32'):\n",
    "        torch.backends.cudnn.allow_tf32 = use_tf32\n",
    "        torch.backends.cuda.matmul.allow_tf32 = use_tf32\n",
    "        \n",
    "    return devices if len(devices) > 1 else devices[0]\n",
    "\n"
   ],
   "metadata": {
    "id": "rj49hLuwR-Tw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def load_UCR(dataset):\n",
    "    train_file = os.path.join('datasets/UCR', dataset, dataset + \"_TRAIN.tsv\")\n",
    "    test_file = os.path.join('datasets/UCR', dataset, dataset + \"_TEST.tsv\")\n",
    "    train_df = pd.read_csv(train_file, sep='\\t', header=None)\n",
    "    test_df = pd.read_csv(test_file, sep='\\t', header=None)\n",
    "    train_array = np.array(train_df)\n",
    "    test_array = np.array(test_df)\n",
    "\n",
    "    # Move the labels to {0, ..., L-1}\n",
    "    labels = np.unique(train_array[:, 0])\n",
    "    transform = {}\n",
    "    for i, l in enumerate(labels):\n",
    "        transform[l] = i\n",
    "\n",
    "    train = train_array[:, 1:].astype(np.float64)\n",
    "    train_labels = np.vectorize(transform.get)(train_array[:, 0])\n",
    "    test = test_array[:, 1:].astype(np.float64)\n",
    "    test_labels = np.vectorize(transform.get)(test_array[:, 0])\n",
    "\n",
    "    if dataset not in [\n",
    "        'AllGestureWiimoteX',\n",
    "        'AllGestureWiimoteY',\n",
    "        'AllGestureWiimoteZ',\n",
    "        'BME',\n",
    "        'Chinatown',\n",
    "        'Crop',\n",
    "        'EOGHorizontalSignal',\n",
    "        'EOGVerticalSignal',\n",
    "        'Fungi',\n",
    "        'GestureMidAirD1',\n",
    "        'GestureMidAirD2',\n",
    "        'GestureMidAirD3',\n",
    "        'GesturePebbleZ1',\n",
    "        'GesturePebbleZ2',\n",
    "        'GunPointAgeSpan',\n",
    "        'GunPointMaleVersusFemale',\n",
    "        'GunPointOldVersusYoung',\n",
    "        'HouseTwenty',\n",
    "        'InsectEPGRegularTrain',\n",
    "        'InsectEPGSmallTrain',\n",
    "        'MelbournePedestrian',\n",
    "        'PickupGestureWiimoteZ',\n",
    "        'PigAirwayPressure',\n",
    "        'PigArtPressure',\n",
    "        'PigCVP',\n",
    "        'PLAID',\n",
    "        'PowerCons',\n",
    "        'Rock',\n",
    "        'SemgHandGenderCh2',\n",
    "        'SemgHandMovementCh2',\n",
    "        'SemgHandSubjectCh2',\n",
    "        'ShakeGestureWiimoteZ',\n",
    "        'SmoothSubspace',\n",
    "        'UMD'\n",
    "    ]:\n",
    "        return train[..., np.newaxis], train_labels, test[..., np.newaxis], test_labels\n",
    "    \n",
    "    mean = np.nanmean(train)\n",
    "    std = np.nanstd(train)\n",
    "    train = (train - mean) / std\n",
    "    test = (test - mean) / std\n",
    "    return train[..., np.newaxis], train_labels, test[..., np.newaxis], test_labels\n",
    "\n",
    "\n",
    "def load_UEA(dataset):\n",
    "    train_data = loadarff(f'datasets/UEA/{dataset}/{dataset}_TRAIN.arff')[0]\n",
    "    test_data = loadarff(f'datasets/UEA/{dataset}/{dataset}_TEST.arff')[0]\n",
    "    \n",
    "    def extract_data(data):\n",
    "        res_data = []\n",
    "        res_labels = []\n",
    "        for t_data, t_label in data:\n",
    "            t_data = np.array([ d.tolist() for d in t_data ])\n",
    "            t_label = t_label.decode(\"utf-8\")\n",
    "            res_data.append(t_data)\n",
    "            res_labels.append(t_label)\n",
    "        return np.array(res_data).swapaxes(1, 2), np.array(res_labels)\n",
    "    \n",
    "    train_X, train_y = extract_data(train_data)\n",
    "    test_X, test_y = extract_data(test_data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_X.reshape(-1, train_X.shape[-1]))\n",
    "    train_X = scaler.transform(train_X.reshape(-1, train_X.shape[-1])).reshape(train_X.shape)\n",
    "    test_X = scaler.transform(test_X.reshape(-1, test_X.shape[-1])).reshape(test_X.shape)\n",
    "    \n",
    "    labels = np.unique(train_y)\n",
    "    transform = { k : i for i, k in enumerate(labels)}\n",
    "    train_y = np.vectorize(transform.get)(train_y)\n",
    "    test_y = np.vectorize(transform.get)(test_y)\n",
    "    return train_X, train_y, test_X, test_y\n",
    "    \n",
    "    \n",
    "def load_forecast_npy(name, univar=False):\n",
    "    data = np.load(f'datasets/{name}.npy')    \n",
    "    if univar:\n",
    "        data = data[: -1:]\n",
    "        \n",
    "    train_slice = slice(None, int(0.6 * len(data)))\n",
    "    valid_slice = slice(int(0.6 * len(data)), int(0.8 * len(data)))\n",
    "    test_slice = slice(int(0.8 * len(data)), None)\n",
    "    \n",
    "    scaler = StandardScaler().fit(data[train_slice])\n",
    "    data = scaler.transform(data)\n",
    "    data = np.expand_dims(data, 0)\n",
    "\n",
    "    pred_lens = [24, 48, 96, 288, 672]\n",
    "    return data, train_slice, valid_slice, test_slice, scaler, pred_lens, 0\n",
    "\n",
    "\n",
    "def _get_time_features(dt):\n",
    "    return np.stack([\n",
    "        dt.minute.to_numpy(),\n",
    "        dt.hour.to_numpy(),\n",
    "        dt.dayofweek.to_numpy(),\n",
    "        dt.day.to_numpy(),\n",
    "        dt.dayofyear.to_numpy(),\n",
    "        dt.month.to_numpy(),\n",
    "        dt.weekofyear.to_numpy(),\n",
    "    ], axis=1).astype(np.float)\n",
    "\n",
    "\n",
    "def load_forecast_csv(name, univar=False):\n",
    "    data = pd.read_csv(f'datasets/{name}.csv', index_col='date', parse_dates=True)\n",
    "    dt_embed = _get_time_features(data.index)\n",
    "    n_covariate_cols = dt_embed.shape[-1]\n",
    "    \n",
    "    if univar:\n",
    "        if name in ('ETTh1', 'ETTh2', 'ETTm1', 'ETTm2'):\n",
    "            data = data[['OT']]\n",
    "        elif name == 'electricity':\n",
    "            data = data[['MT_001']]\n",
    "        else:\n",
    "            data = data.iloc[:, -1:]\n",
    "        \n",
    "    data = data.to_numpy()\n",
    "    if name == 'ETTh1' or name == 'ETTh2':\n",
    "        train_slice = slice(None, 12*30*24)\n",
    "        valid_slice = slice(12*30*24, 16*30*24)\n",
    "        test_slice = slice(16*30*24, 20*30*24)\n",
    "    elif name == 'ETTm1' or name == 'ETTm2':\n",
    "        train_slice = slice(None, 12*30*24*4)\n",
    "        valid_slice = slice(12*30*24*4, 16*30*24*4)\n",
    "        test_slice = slice(16*30*24*4, 20*30*24*4)\n",
    "    else:\n",
    "        train_slice = slice(None, int(0.6 * len(data)))\n",
    "        valid_slice = slice(int(0.6 * len(data)), int(0.8 * len(data)))\n",
    "        test_slice = slice(int(0.8 * len(data)), None)\n",
    "    \n",
    "    scaler = StandardScaler().fit(data[train_slice])\n",
    "    data = scaler.transform(data)\n",
    "    if name in ('electricity'):\n",
    "        data = np.expand_dims(data.T, -1)  # Each variable is an instance rather than a feature\n",
    "    else:\n",
    "        data = np.expand_dims(data, 0)\n",
    "    \n",
    "    if n_covariate_cols > 0:\n",
    "        dt_scaler = StandardScaler().fit(dt_embed[train_slice])\n",
    "        dt_embed = np.expand_dims(dt_scaler.transform(dt_embed), 0)\n",
    "        data = np.concatenate([np.repeat(dt_embed, data.shape[0], axis=0), data], axis=-1)\n",
    "    \n",
    "    if name in ('ETTh1', 'ETTh2', 'electricity'):\n",
    "        pred_lens = [24, 48, 168, 336, 720]\n",
    "    else:\n",
    "        pred_lens = [24, 48, 96, 288, 672]\n",
    "        \n",
    "    return data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols\n",
    "\n",
    "\n",
    "def load_anomaly(name):\n",
    "    res = pkl_load(f'datasets/{name}.pkl')\n",
    "    return res['all_train_data'], res['all_train_labels'], res['all_train_timestamps'], \\\n",
    "           res['all_test_data'],  res['all_test_labels'],  res['all_test_timestamps'], \\\n",
    "           res['delay']\n",
    "\n",
    "\n",
    "def gen_ano_train_data(all_train_data):\n",
    "    maxl = np.max([ len(all_train_data[k]) for k in all_train_data ])\n",
    "    pretrain_data = []\n",
    "    for k in all_train_data:\n",
    "        train_data = pad_nan_to_target(all_train_data[k], maxl, axis=0)\n",
    "        pretrain_data.append(train_data)\n",
    "    pretrain_data = np.expand_dims(np.stack(pretrain_data), 2)\n",
    "    return pretrain_data\n"
   ],
   "metadata": {
    "id": "WEfZn_ElR1bO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "id": "emajP-VnUAD1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_excel('EuqityMOEXReturns.xlsx', sheet_name='D')\n",
    "df = df.drop(columns=df.columns[df.isna().sum() > len(df) / 2])\n",
    "df = df.dropna()\n",
    "df = df.rename(columns={'Unnamed: 0': 'date'})\n",
    "df['date'] = df['date'].dt.date\n",
    "df = df.sort_values(by='date').reset_index()\n",
    "df.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuQ-5fShT5Pw",
    "outputId": "a011c6a7-0781-4149-e968-adf6607469ea"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(899, 69)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "def fit_svm(features, y, MAX_SAMPLES=10000):\n",
    "    nb_classes = np.unique(y, return_counts=True)[1].shape[0]\n",
    "    train_size = features.shape[0]\n",
    "\n",
    "    svm = SVC(C=np.inf, gamma='scale')\n",
    "    if train_size // nb_classes < 5 or train_size < 50:\n",
    "        return svm.fit(features, y)\n",
    "    else:\n",
    "        grid_search = GridSearchCV(\n",
    "            svm, {\n",
    "                'C': [\n",
    "                    0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000,\n",
    "                    np.inf\n",
    "                ],\n",
    "                'kernel': ['rbf'],\n",
    "                'degree': [3],\n",
    "                'gamma': ['scale'],\n",
    "                'coef0': [0],\n",
    "                'shrinking': [True],\n",
    "                'probability': [False],\n",
    "                'tol': [0.001],\n",
    "                'cache_size': [200],\n",
    "                'class_weight': [None],\n",
    "                'verbose': [False],\n",
    "                'max_iter': [10000000],\n",
    "                'decision_function_shape': ['ovr'],\n",
    "                'random_state': [None]\n",
    "            },\n",
    "            cv=5, n_jobs=5\n",
    "        )\n",
    "        # If the training set is too large, subsample MAX_SAMPLES examples\n",
    "        if train_size > MAX_SAMPLES:\n",
    "            split = train_test_split(\n",
    "                features, y,\n",
    "                train_size=MAX_SAMPLES, random_state=0, stratify=y\n",
    "            )\n",
    "            features = split[0]\n",
    "            y = split[2]\n",
    "            \n",
    "        grid_search.fit(features, y)\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "def fit_lr(features, y, MAX_SAMPLES=100000):\n",
    "    # If the training set is too large, subsample MAX_SAMPLES examples\n",
    "    if features.shape[0] > MAX_SAMPLES:\n",
    "        split = train_test_split(\n",
    "            features, y,\n",
    "            train_size=MAX_SAMPLES, random_state=0, stratify=y\n",
    "        )\n",
    "        features = split[0]\n",
    "        y = split[2]\n",
    "        \n",
    "    pipe = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(\n",
    "            random_state=0,\n",
    "            max_iter=1000000,\n",
    "            multi_class='ovr'\n",
    "        )\n",
    "    )\n",
    "    pipe.fit(features, y)\n",
    "    return pipe\n",
    "\n",
    "def fit_knn(features, y):\n",
    "    pipe = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        KNeighborsClassifier(n_neighbors=1)\n",
    "    )\n",
    "    pipe.fit(features, y)\n",
    "    return pipe\n",
    "\n",
    "def fit_ridge(train_features, train_y, valid_features, valid_y, MAX_SAMPLES=100000):\n",
    "    # If the training set is too large, subsample MAX_SAMPLES examples\n",
    "    if train_features.shape[0] > MAX_SAMPLES:\n",
    "        split = train_test_split(\n",
    "            train_features, train_y,\n",
    "            train_size=MAX_SAMPLES, random_state=0\n",
    "        )\n",
    "        train_features = split[0]\n",
    "        train_y = split[2]\n",
    "    if valid_features.shape[0] > MAX_SAMPLES:\n",
    "        split = train_test_split(\n",
    "            valid_features, valid_y,\n",
    "            train_size=MAX_SAMPLES, random_state=0\n",
    "        )\n",
    "        valid_features = split[0]\n",
    "        valid_y = split[2]\n",
    "    \n",
    "    alphas = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "    valid_results = []\n",
    "    for alpha in alphas:\n",
    "        lr = Ridge(alpha=alpha).fit(train_features, train_y)\n",
    "        valid_pred = lr.predict(valid_features)\n",
    "        score = np.sqrt(((valid_pred - valid_y) ** 2).mean()) + np.abs(valid_pred - valid_y).mean()\n",
    "        valid_results.append(score)\n",
    "    best_alpha = alphas[np.argmin(valid_results)]\n",
    "    \n",
    "    lr = Ridge(alpha=best_alpha)\n",
    "    lr.fit(train_features, train_y)\n",
    "    return lr"
   ],
   "metadata": {
    "id": "dHOq9AWyEmm_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def generate_pred_samples(features, data, pred_len, drop=0):\n",
    "    n = data.shape[1]\n",
    "    features = features[:, :-pred_len]\n",
    "    labels = np.stack([ data[:, i:1+n+i-pred_len] for i in range(pred_len)], axis=2)[:, 1:]\n",
    "    features = features[:, drop:]\n",
    "    labels = labels[:, drop:]\n",
    "    return features.reshape(-1, features.shape[-1]), \\\n",
    "            labels.reshape(-1, labels.shape[2]*labels.shape[3])\n",
    "\n",
    "def cal_metrics(pred, target):\n",
    "    return {\n",
    "        'MSE': ((pred - target) ** 2).mean(),\n",
    "        'MAE': np.abs(pred - target).mean()\n",
    "    }\n",
    "    \n",
    "def eval_forecasting(model, data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols):\n",
    "    padding = 200\n",
    "    \n",
    "    t = time.time()\n",
    "    all_repr = model.encode(\n",
    "        data,\n",
    "        casual=True,\n",
    "        sliding_length=1,\n",
    "        sliding_padding=padding,\n",
    "        batch_size=256\n",
    "    )\n",
    "    ts2vec_infer_time = time.time() - t\n",
    "    \n",
    "    train_repr = all_repr[:, train_slice]\n",
    "    valid_repr = all_repr[:, valid_slice]\n",
    "    test_repr = all_repr[:, test_slice]\n",
    "    \n",
    "    train_data = data[:, train_slice, n_covariate_cols:]\n",
    "    valid_data = data[:, valid_slice, n_covariate_cols:]\n",
    "    test_data = data[:, test_slice, n_covariate_cols:]\n",
    "    \n",
    "    ours_result = {}\n",
    "    lr_train_time = {}\n",
    "    lr_infer_time = {}\n",
    "    out_log = {}\n",
    "    for pred_len in pred_lens:\n",
    "        train_features, train_labels = generate_pred_samples(train_repr, train_data, pred_len, drop=padding)\n",
    "        valid_features, valid_labels = generate_pred_samples(valid_repr, valid_data, pred_len)\n",
    "        test_features, test_labels = generate_pred_samples(test_repr, test_data, pred_len)\n",
    "        \n",
    "        t = time.time()\n",
    "        lr = eval_protocols.fit_ridge(train_features, train_labels, valid_features, valid_labels)\n",
    "        lr_train_time[pred_len] = time.time() - t\n",
    "        \n",
    "        t = time.time()\n",
    "        test_pred = lr.predict(test_features)\n",
    "        lr_infer_time[pred_len] = time.time() - t\n",
    "\n",
    "        ori_shape = test_data.shape[0], -1, pred_len, test_data.shape[2]\n",
    "        test_pred = test_pred.reshape(ori_shape)\n",
    "        test_labels = test_labels.reshape(ori_shape)\n",
    "        \n",
    "        if test_data.shape[0] > 1:\n",
    "            test_pred_inv = scaler.inverse_transform(test_pred.swapaxes(0, 3)).swapaxes(0, 3)\n",
    "            test_labels_inv = scaler.inverse_transform(test_labels.swapaxes(0, 3)).swapaxes(0, 3)\n",
    "        else:\n",
    "            test_pred_inv = scaler.inverse_transform(test_pred)\n",
    "            test_labels_inv = scaler.inverse_transform(test_labels)\n",
    "            \n",
    "        out_log[pred_len] = {\n",
    "            'norm': test_pred,\n",
    "            'raw': test_pred_inv,\n",
    "            'norm_gt': test_labels,\n",
    "            'raw_gt': test_labels_inv\n",
    "        }\n",
    "        ours_result[pred_len] = {\n",
    "            'norm': cal_metrics(test_pred, test_labels),\n",
    "            'raw': cal_metrics(test_pred_inv, test_labels_inv)\n",
    "        }\n",
    "        \n",
    "    eval_res = {\n",
    "        'ours': ours_result,\n",
    "        'ts2vec_infer_time': ts2vec_infer_time,\n",
    "        'lr_train_time': lr_train_time,\n",
    "        'lr_infer_time': lr_infer_time\n",
    "    }\n",
    "    return out_log, eval_res"
   ],
   "metadata": {
    "id": "UyuajOJaEnRa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TlX5B4JPE1Wd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = TS2Vec(\n",
    "    input_dims=1,\n",
    "    device=0,\n",
    "    output_dims=10\n",
    ")"
   ],
   "metadata": {
    "id": "EMBpLVUBJXv_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# work!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
